services:
  chroma:
    image: chromadb/chroma:1.0.15
    volumes:
      - chroma_data:/data
    ports:
      - "8000:8000"
    networks:
      - internal

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data



  document-ai-api:
   image: danitherex/document-ai-api:latest
   profiles: ["remote"]
   depends_on:
     - chroma
   environment:
     - EMBEDDING_SERVICE=huggingface
     - LLM_SERVICE=qwen
     - DASHSCOPE_API_KEY=KEY
     - HUGGINGFACE_API_KEY=KEY
     #- GROQ_API_KEY=DUMMY_API_KEY
     - EMBEDDING_MODEL_NAME=Qwen/Qwen3-Embedding-0.6B
     - LLM_MODEL_NAME=qwen3-32b
     - DOMAIN=http://localhost:3000

     - CHROMA_HOST=chroma
     - CHROMA_PORT=8000
   ports:
    - "8008:8008"
   networks:
     - internal


  document-ai-api-local:
    profiles: ["local-gpu"]
    build:
      context: .
      dockerfile: Dockerfile.local
    #runtime: nvidia                            # enable GPU runtime
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                - gpu
    depends_on:
      - chroma
    environment:
     - EMBEDDING_SERVICE=huggingface
     - LLM_SERVICE=qwen_local
     #- DASHSCOPE_API_KEY=KEY
     - HUGGINGFACE_API_KEY=KEY
     #- GROQ_API_KEY=DUMMY_API_KEY
     - EMBEDDING_MODEL_NAME=Qwen/Qwen3-Embedding-0.6B
     - DOMAIN=http://localhost:3000

     - CHROMA_HOST=chroma
     - CHROMA_PORT=8000
    volumes:
      - models:/models                         # persist GGUF between rebuilds
    ports:
      - "8008:8008"
    networks:
      - internal



networks:
  internal:

volumes:
  chroma_data:
  open-webui:
  models:
