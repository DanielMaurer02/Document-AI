################ 1. build stage (compile llama‑cpp‑python) ################
FROM nvidia/cuda:12.5.0-devel-ubuntu22.04 AS builder # TODO: Change to correct CUDA version

ARG EMBEDDING_SERVICE
ARG LLM_SERVICE
ARG EMBEDDING_MODEL_NAME
ARG CHROMA_HOST
ARG CHROMA_PORT
ARG DOMAIN

ENV EMBEDDING_SERVICE=${EMBEDDING_SERVICE}
ENV LLM_SERVICE=${LLM_SERVICE}
ENV EMBEDDING_MODEL_NAME=${EMBEDDING_MODEL_NAME}
ENV CHROMA_HOST=${CHROMA_HOST}
ENV CHROMA_PORT=${CHROMA_PORT}
ENV DOMAIN=${DOMAIN}
ENV TOKENIZERS_PARALLELISM=False


# system deps
RUN apt-get update && \
    apt-get install -y --no-install-recommends curl git build-essential cmake && \
    rm -rf /var/lib/apt/lists/*

# uv – single‑binary installer
RUN curl -Ls https://astral.sh/uv/install | bash
ENV PATH="/root/.local/bin:$PATH" \
    FORCE_CMAKE=1 \
    CMAKE_ARGS="-DGGML_CUDA=ON -DLLAMA_CUBLAS=ON" \
    UV_LINK_MODE=copy UV_COMPILE_BYTECODE=1 UV_PYTHON_DOWNLOADS=0

WORKDIR /app
COPY pyproject.toml uv.lock ./

# build‑only extras first (CUDA wheel)
RUN uv sync --locked --group build --no-dev
# then runtime deps
RUN uv sync --locked --no-install-project --no-dev

COPY . .

# pull the GGUF weights once
RUN uv run python scripts/download_model.py

################ 2. runtime stage (thin, CUDA libs only) ##################
FROM nvidia/cuda:12.5.0-runtime-ubuntu22.04
WORKDIR /app
COPY --from=builder /app /app
ENV PATH="/app/.venv/bin:$PATH" \
    TOKENIZERS_PARALLELISM=false

EXPOSE 8008
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8008"]
